# Sign-language-translator
dynamic sign language translator
Project Documentation
________________________________________
Project Overview
The project focuses on creating a sign language recognition system that integrates real-time gesture detection and recognition using machine learning, webcam inputs, and user-friendly interaction. The system will also include speech synthesis to translate recognized gestures into spoken words, providing an accessible and interactive platform.
________________________________________
Features
1.	Real-Time Gesture Recognition:
  It will use TensorFlow.js and a kNN classifier to detect and classify hand gestures.
o	It will implement a training system to add new gesture examples dynamically.
3.	Webcam Integration:
o	Utilizes the getUserMedia API for video feed access.
o	Captures video frames using the Canvas API for processing.
4.	User-Friendly Interface:
   It will feature an intuitive UI for gesture training, feedback, and interaction.
o	Display captured images with user-provided labels.
5.	Speech Synthesis:
o	pending work
6.	Video Call Integration:
o	pending
7.	Responsive Design:
o	The UI aims to adapt to various devices, ensuring a seamless user experience.
________________________________________
Technology Stack
•	Frontend:
o	HTML, CSS, JavaScript
o	Responsive design with Flexbox
•	Machine Learning:
o	TensorFlow.js for kNN classifier
•	APIs and Libraries:
o	getUserMedia API for webcam integration
o	Canvas API for frame capturing
•	Deployment:
o	GitHub 
________________________________________


