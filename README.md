# Sign-language-translator
dynamic sign language translator
Project Documentation
________________________________________
Project Overview
The project focuses on creating a sign language recognition system that integrates real-time gesture detection and recognition using machine learning, webcam inputs, and user-friendly interaction. The system also includes speech synthesis to translate recognized gestures into spoken words, providing an accessible and interactive platform.
________________________________________
Features
1.	Real-Time Gesture Recognition:
o	Uses TensorFlow.js and a kNN classifier to detect and classify hand gestures.
o	Implements a training system to add new gesture examples dynamically.
2.	Webcam Integration:
o	Utilizes the getUserMedia API for video feed access.
o	Captures video frames using the Canvas API for processing.
3.	User-Friendly Interface:
o	Features an intuitive UI for gesture training, feedback, and interaction.
o	Displays captured images with user-provided labels.
4.	Speech Synthesis:
o	Converts recognized gestures into speech using the Speech Synthesis API.
5.	Video Call Integration:
o	Supports real-time video calling using WebRTC.
o	Enables gesture recognition within video calls.
6.	Responsive Design:
o	The UI adapts to various devices, ensuring a seamless user experience.
________________________________________
Technology Stack
•	Frontend:
o	HTML, CSS, JavaScript
o	Responsive design with Flexbox
•	Machine Learning:
o	TensorFlow.js for kNN classifier
•	APIs and Libraries:
o	getUserMedia API for webcam integration
o	Canvas API for frame capturing
o	Speech Synthesis API for text-to-speech conversion
o	WebRTC for video calling
•	Deployment:
o	GitHub Pages, Firebase, or Netlify for hosting
________________________________________


